

<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" /><title>Code预训练语言模型学习指南（原理/分析/代码）Part2 - Qiushi</title><meta name="Description" content=""><meta property="og:title" content="Code预训练语言模型学习指南（原理/分析/代码）Part2" />
<meta property="og:description" content="Code预训练语言模型学习指南（原理/分析/代码）Part2" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://qiushisun.github.io/blog-posts/posts/2022-7-codeptms-review2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-09T20:00:03+00:00" />
<meta property="article:modified_time" content="2022-07-09T20:00:03+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Code预训练语言模型学习指南（原理/分析/代码）Part2"/>
<meta name="twitter:description" content="Code预训练语言模型学习指南（原理/分析/代码）Part2"/>
      <meta name="twitter:site" content="@qiushi_sun"/>
<meta name="application-name" content="DoIt">
<meta name="apple-mobile-web-app-title" content="DoIt">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><meta name="twitter:creator" content="@qiushi_sun" /><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<link rel="canonical" href="https://qiushisun.github.io/blog-posts/posts/2022-7-codeptms-review2/" /><link rel="prev" href="https://qiushisun.github.io/blog-posts/posts/2022-7-codeptms-review1/" /><link rel="next" href="https://qiushisun.github.io/blog-posts/posts/first_post/" />
<link rel="stylesheet" href="/blog-posts/css/main.min.css"><link rel="stylesheet" href="/blog-posts/css/style.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/blog-posts/lib/animate/animate.min.css">
        <noscript><link rel="stylesheet" href="/blog-posts/lib/animate/animate.min.css"></noscript><script type="application/ld+json">{"@context": "https://schema.org","@type": "BlogPosting",
        "headline": "Code预训练语言模型学习指南（原理/分析/代码）Part2",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://qiushisun.github.io/blog-posts/posts/2022-7-codeptms-review2/"
        },"genre": "posts","keywords":["CodePTMs"],"wordcount":  227 ,
        "url": "https://qiushisun.github.io/blog-posts/posts/2022-7-codeptms-review2/","datePublished": "2022-07-09T20:00:03+00:00","dateModified": "2022-07-09T20:00:03+00:00","publisher": {
            "@type": "Organization",
            "name": "Author"},"author": {
                "@type": "Person",
                "name": "Author",
                "url": "/blog-posts/"
            },"description": ""
    }</script></head>

<body header-desktop="" header-mobile=""><script type="text/javascript">
        function setTheme(theme) {
          document.body.setAttribute('theme', theme); 
          document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark');
          if (theme === 'light') {
            document.documentElement.classList.remove('tw-dark')
          } else {
            document.documentElement.classList.add('tw-dark')
          }
          window.theme = theme;   
          window.isDark = window.theme !== 'light' 
        }
        function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
        function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
        if (window.localStorage && localStorage.getItem('theme')) {let theme = localStorage.getItem('theme');theme === 'light' || theme === 'dark' || theme === 'black' ? setTheme(theme) : (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light')); } else { if ('' === 'light' || '' === 'dark' || '' === 'black') setTheme(''), saveTheme(''); else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');}
        let metaColors = {'light': '#f8f8f8','dark': '#252627','black': '#000000'}
        getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
        window.switchThemeEventSet = new Set()
    </script>
    <div id="back-to-top"></div>
    <div id="mask"></div><div class="wrapper"><header class="desktop print:!tw-hidden" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/blog-posts/" title="Qiushi">Qiushi</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item"
                    href="/blog-posts/links/" > Friends </a><a class="menu-item"
                    href="/blog-posts/about/" > About </a><span class="menu-item delimiter"></span><button class="menu-item theme-switch" aria-label="Switch Theme">
                    <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
                </button></div>
        </div>
    </div>
</header><header class="mobile print:!tw-hidden" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/blog-posts/" title="Qiushi">Qiushi</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/blog-posts/links/" title="" >Friends</a><a class="menu-item" href="/blog-posts/about/" title="" >About</a><button class="menu-item theme-switch tw-w-full" aria-label="Switch Theme">
                <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M8 256c0 136.966 111.033 248 248 248s248-111.034 248-248S392.966 8 256 8 8 119.033 8 256zm248 184V72c101.705 0 184 82.311 184 184 0 101.705-82.311 184-184 184z"/></svg>
            </button></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="main">
            <div class="container"><article class="page single print:!tw-w-full print:!tw-max-w-none print:!tw-m-0 print:!tw-p-0"><h1 class="single-title animate__animated animate__flipInX">Code预训练语言模型学习指南（原理/分析/代码）Part2</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><a href="/blog-posts/" title="Author" rel=" author" class="author">Author</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">categories <a href="/blog-posts/categories/nlp/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>NLP</a>&nbsp;<a href="/blog-posts/categories/code-intelligence/"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>Code Intelligence</a></span></div>
            <div class="post-meta-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;<time datetime="2022-07-09">2022-07-09</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1 0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7 0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174L402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7l-43.2-43.2c-4.1-4.1-10.8-4.1-14.8 0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg>&nbsp;<time datetime="2022-07-09">2022-07-09</time>&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;227 words&nbsp;<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;2 minutes&nbsp;</div>
        </div><div class="content" id="content"><p>Code预训练语言模型学习指南（原理/分析/代码）Part2</p>
<p>这个主题的上一篇文章 <a href="https://qiushisun.github.io/2022/07/06/2022-7-CodePTMs-Review1/" target="_blank" rel="noopener noreferrer">Code预训练语言模型学习指南（原理/分析/代码）Part1</a> 讲解了几个典型的Code预训练语言模型（CodePTMs），这篇文章顺延这个话题，继续讨论21年及以后出现的CodePTMs，并且补充一些代码。</p>
<h2 id="plbart" class="headerLink">
    <a href="#plbart" class="header-mark"></a>PLBART</h2><p>NAACL 2021 <a href="https://aclanthology.org/2021.naacl-main.211/" target="_blank" rel="noopener noreferrer">Unified Pre-training for Program Understanding and Generation</a></p>
<p>顾名思义，就是应用于编程语言的BART，参考了ACL 2020的文章：<a href="https://arxiv.org/pdf/1910.13461.pdf" target="_blank" rel="noopener noreferrer">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></p>
<p>先简单说下BART，它吸收了BERT模型的双向编码器和GPT中的单向left-to-right解码器这二者的优点，因此比BERT更适合文本生成的场景（BERT因Pre-training阶段和生成式下游任务差异比较大，因此被认为不适合做NLG相关任务），而它相比GPT，也多了双向上下文语境信息（GPT是单向建模）。除此之外，相比BERT的Token Masking，BART对Encoder端采用了更加复杂的Noise。</p>
<p>基于对BART的知识，理解PLBART并不算困难，其使用了和BART-base相同的架构，唯一结构上的不同的是它在Encoder和Decoder的顶部添加了一个额外的LayerNorm。在Noise策略方面，PLBART使用了token masking, token deletion和token infilling这三种策略，与BART相比少了Sentence Permutation和Document Rotation这两个任务，这些任务的细节都可以参考BART原文。</p>
<p>在微调下游任务方面，作者将PLBART的下游任务分为两块</p>
<p>首先是Sequence Generation，又可细分为三个任务，可参考下图</p>
<p><figure><img
        
        loading="lazy"
        src="2022-7-CodePTMs-Review/PLBART-Seq-Gen.png"
        srcset="2022-7-CodePTMs-Review/PLBART-Seq-Gen.png, 2022-7-CodePTMs-Review/PLBART-Seq-Gen.png 1.5x, 2022-7-CodePTMs-Review/PLBART-Seq-Gen.png 2x"
        title="PLBART: Sequence Generation" ><figcaption class="image-caption">PLBART: Sequence Generation</figcaption>
    </figure></p>
<p>其次是Sequence Classification：将序列尾部的special token喂给线性分类器用于预测，与BERT等模型的分类区别不大。</p>
<p>实验与比较方面，作者先指定了baseline模型，并将其分成了两种</p>
<p>Training from Scratch，作者用下游任务的数据集从零开始训练了LSTM+Attention以及一个Transformer
Pre-trained Models，作者挑选了RoBERTa、CodeBERT、GraphCodeBERT、GPT-2、CodeGPT(-adapted)
具体的实验部分做了Code Summarization、Code Generation、Code Translation这三个生成式任务，效果自然是好的，在Classification方面做了两个任务：clone detection和vulnerability detection，在后者上PLBART不如基于AST的模型。</p>
<h2 id="codet5" class="headerLink">
    <a href="#codet5" class="header-mark"></a>CodeT5</h2><p>EMNLP 2021 <a href="https://aclanthology.org/2021.emnlp-main.685/" target="_blank" rel="noopener noreferrer">CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</a></p>
<p>文中对CodeT5的描述是：a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers，即一个能更好地利用代码语法信息（形式是identifier，即标识符）的统一预训练Transformer模型。在开始之前，和PLBART一样，先简单说下Google T5模型。T5的名字来源是Text-To-Text Transfer Transformer，顾名思义T5把所有的NLP问题统一归纳为了Text2Text任务，用来做NMT、QA、文本摘要和文本分类任务。</p>
<p><figure><img
        
        loading="lazy"
        src="2022-7-CodePTMs-Review/T5-Illustration.png"
        srcset="2022-7-CodePTMs-Review/T5-Illustration.png, 2022-7-CodePTMs-Review/T5-Illustration.png 1.5x, 2022-7-CodePTMs-Review/T5-Illustration.png 2x"
        title="T5-Illustration" ><figcaption class="image-caption">T5-Illustration</figcaption>
    </figure></p>
<p>对比下T5原文，可以发现二者的核心思想还是非常类似的，作者将CodeT5归纳为a pre-trained encoder-decoder model that considers the token type information in code，细心的玩家可能发现了，前面提到的CodeBERT为首的BERT类模型和CodeGPT为首的GPT类模型，仅含有Encoder或Decoder，而非完整利用一个Transformer架构来处理代码。因此CodeT5最大的卖点即第一个unified encoder-decoder CodePTM，可以理解为完全使用了Transformer的两个部分。</p>
<p><figure><img
        
        loading="lazy"
        src="2022-7-CodePTMs-Review/CodeT5-Illustration.png"
        srcset="2022-7-CodePTMs-Review/CodeT5-Illustration.png, 2022-7-CodePTMs-Review/CodeT5-Illustration.png 1.5x, 2022-7-CodePTMs-Review/CodeT5-Illustration.png 2x"
        title="CodeT5-Illustration" ><figcaption class="image-caption">CodeT5-Illustration</figcaption>
    </figure></p>
<p>此外，除了使用T5的架构外，作者使用了以下两个方案来更好地利用代码结构特性：</p>
<p>使用了代码的标识符信息，提出了Identifier-aware Pre-training，是一个与T5中Masked Span类似的目标，简而言之就是随机mask掉任意长度的Span，然后让decoder去预测。
利用了代码地Comments（自然语言注释）信息，作者称之为Bimodel Dual Generation，让自然语言和源代码的表征可以对齐，帮助缓和Pre-training和Fine-Tuning阶段的差距。</p>
<p><figure><img
        
        loading="lazy"
        src="2022-7-CodePTMs-Review/CodeT5-pretraining-task.png"
        srcset="2022-7-CodePTMs-Review/CodeT5-pretraining-task.png, 2022-7-CodePTMs-Review/CodeT5-pretraining-task.png 1.5x, 2022-7-CodePTMs-Review/CodeT5-pretraining-task.png 2x"
        title="Identifier-aware Pre-training" ><figcaption class="image-caption">CodeT5-pretraining</figcaption>
    </figure></p>
<p>文章发表时在CodeXGLUE Benchmark的若干任务上取得了SOTA效果。</p>
<h2 id="unixcoder" class="headerLink">
    <a href="#unixcoder" class="header-mark"></a>UniXcoder</h2><p>ACL 2022 <a href="https://aclanthology.org/2022.acl-long.499/" target="_blank" rel="noopener noreferrer">UniXcoder: Unified Cross-Modal Pre-training for Code Representation</a></p>
<p>这是当今（2022.7）的SOTA模型，参考了NIPS2019: Unified Language Model Pre-training for Natural Language Understanding and Generation</p>
<p>文章选择了五个任务，分为以下三类</p>
<p>代码理解任务：Clone Detection和Code Search
代码生成任务：Code Summary和Code Generation
自回归任务：Code Completion
本文很重要的一个卖点就是更全面地利用了AST提供的代码结构信息。文章开头讲过，AST一般会被表示为一个Tree结构，不能直接作为Transformer类模型的输入，回忆一下前面提到的GraphCodeBERT，作者在以损失相当一部分信息的情况下让模型学习AST的数据流。为了能更加有效地利用AST，因此UniXcoder地作者构建了一个one-to-one的mapping function，将AST转为一个序列结构（flattened token sequence），然后和Code Comments一同编码，对这个mapping function的有效性的证明在文章的附录中。</p>
<p>模型结构方面，UniXcoder的一个卖点就是一个统一的，可以同时兼容Encoder-Only，Decoder-Only和Encoder-Decoder三种模式的CodePTM，相当于给模型添加了“开关”，来决定采用什么模式处理任务，用白话讲，就是通过使用三种不同类型的自注意力Mask策略来控制模型的行为。</p>
<p><figure><img
        
        loading="lazy"
        src="2022-6-10-UniXcoder/unixcoder-arch.png"
        srcset="2022-6-10-UniXcoder/unixcoder-arch.png, 2022-6-10-UniXcoder/unixcoder-arch.png 1.5x, 2022-6-10-UniXcoder/unixcoder-arch.png 2x"
        title="UniXcoder的架构" ><figcaption class="image-caption">UniXcoder's Architecture</figcaption>
    </figure></p>
<p>既然同时能拥有三种模式，那么自然会有更多预训练任务，如下所示：</p>
<p>Masked Language Modeling（MLM），算是基本操作了。
Unidirectional Language Modeling（ULM），用于训练decoder-only模式，帮助完成自回归任务，对应的是右上三角masking。
Denoising Objective DeNoiSing (DNS)，可参考BART和T5，用于训练encoder-decoder模式，帮助完成生成任务，参考架构图中的encoder-decoder部分。
除了上面这些任务以外，作者还提出了Code Fragment Representation Learning</p>
<p><figure><img
        
        loading="lazy"
        src="2022-6-10-UniXcoder/unixcoder-Code-fragment-representation-learning.png"
        srcset="2022-6-10-UniXcoder/unixcoder-Code-fragment-representation-learning.png, 2022-6-10-UniXcoder/unixcoder-Code-fragment-representation-learning.png 1.5x, 2022-6-10-UniXcoder/unixcoder-Code-fragment-representation-learning.png 2x"
        title="Code Fragment Representation Learning" ><figcaption class="image-caption">Code Fragment Representation Learning</figcaption>
    </figure></p>
<p>其中包含了Multi-modal Contrastive Learning (MCL) 和 Cross-Modal Generation (CMG)这两个任务。前者采用了一个对比学习损失，后者是使用了一个统一的自然语言描述（comment），文中使用了fulcrum，即支点这个词，让模型学习到的代码表征在不同语言之间的对齐。</p>
<p>还需注意的一点就是，UniXcoder在预训练和微调这两个阶段中的输入形式有所不同，由于引入了Flattened AST，AST展开后的序列中被引入了大量额外的tokens（70% longer）会导致额外的开销。因此，在微调阶段UniXcoder仅使用AST的叶子节点，为了缓解这个gap，在预训练阶段作者设置了0.5的概率随机丢弃输入序列中的非叶子节点。</p>
<p>除了Clone Detection、Code Search、Code Summarization和Code Completion等任务上表现较好外，UniXcoder还提供了一个新任务：zero-shot code-to-code search，即在zero-shot的情境下，通过一个源代码的query在candidates集合中寻找语义相同的源代码，该任务使用的数据集是<a href="https://arxiv.org/abs/2105.12655v1" target="_blank" rel="noopener noreferrer">CodeNet</a>，2021年的一篇数据集文章，用来衡量训练所得的code fragment embeddings的效果。</p>
<p><figure><img
        
        loading="lazy"
        src="2022-6-10-UniXcoder/unixcoder-zero-shot-code2code-search.png"
        srcset="2022-6-10-UniXcoder/unixcoder-zero-shot-code2code-search.png, 2022-6-10-UniXcoder/unixcoder-zero-shot-code2code-search.png 1.5x, 2022-6-10-UniXcoder/unixcoder-zero-shot-code2code-search.png 2x"
        title="zero-shot code-to-code search" ><figcaption class="image-caption">zero-shot code-to-code search</figcaption>
    </figure></p>
<h2 id="相关代码整理" class="headerLink">
    <a href="#%e7%9b%b8%e5%85%b3%e4%bb%a3%e7%a0%81%e6%95%b4%e7%90%86" class="header-mark"></a>相关代码整理</h2><h3 id="cubert" class="headerLink">
    <a href="#cubert" class="header-mark"></a>CuBERT</h3><p><a href="https://github.com/google-research/google-research/tree/master/cubert" target="_blank" rel="noopener noreferrer">https://github.com/google-research/google-research/tree/master/cubert</a></p>
<h3 id="codebertgraphcodebert和unixcoder" class="headerLink">
    <a href="#codebertgraphcodebert%e5%92%8cunixcoder" class="header-mark"></a>CodeBERT、GraphCodeBERT和UniXCoder</h3><p>MSRA提供了CodeBERT、GraphCodeBERT和UniXCoder在下游任务微调时可用的代码，在仓库<a href="github.com/microsoft/CodeBERT" rel="">https://github.com/microsoft/</a>CodeBERT，但没有提供预训练阶段的实现（CodeBERT和UniXCoder在预训练阶段都使用了16 张 32GB NVIDIA Tesla V100实现），使用时通过transformers加载checkpoints就可使用。此外，huggingface还提供了一个经济适用版的CodeBERT模型：<a href="https://huggingface.co/huggingface/CodeBERTa-small-v1" target="_blank" rel="noopener noreferrer">huggingface/CodeBERTa-small-v1</a></p>
<h3 id="codegpt" class="headerLink">
    <a href="#codegpt" class="header-mark"></a>CodeGPT</h3><p>与上述三个MSRA提供的模型一样，CodeGPT仍然是提供了可通过transformers加载checkpoints，即<a href="https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2" target="_blank" rel="noopener noreferrer">CodeGPT-small-java-adaptedGPT2</a></p>
<h3 id="codet5-1" class="headerLink">
    <a href="#codet5-1" class="header-mark"></a>CodeT5</h3><p><a href="https://github.com/salesforce/codet5" target="_blank" rel="noopener noreferrer">https://github.com/salesforce/codet5</a></p>
<h3 id="plbart-1" class="headerLink">
    <a href="#plbart-1" class="header-mark"></a>PLBART</h3><p><a href="https://github.com/wasiahmad/PL" target="_blank" rel="noopener noreferrer">https://github.com/wasiahmad/PL</a></p>
<h2 id="总结" class="headerLink">
    <a href="#%e6%80%bb%e7%bb%93" class="header-mark"></a>总结</h2><p>上述对CodePTMs相关的内容大致上是22年六月中下旬赶一个文章时调研文献后总结的笔记，然后补充了点链接和细节，实验部分写的比较简略，是因为如果每个模型的实验都要全讲的话篇幅就太长了，而且这些任务都大差不差，每个模型都讲一遍冗余会比较多，之后可能会在其他文章补充。此外，有一些影响力比较小或者Task-specific的工作可能没完全覆盖到。总而言之，不论是CodeBERT开的新坑还是今天的SOTA模型UniXcoder，MSRA在这个领域还是完全dominant的存在。对于CodeBERT和GraphCodeBERT为首的大模型，复现预训练阶段的成本很高，不适合平民玩家，而且今年五月的IJCAI 22 Survey Track连CodePTMs的Survey工作都已经出了（Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code），可能短时间内出革命性的新模型的可能性不大，而且Code领域使用的这些方法终究还是跟着NLP走的，需要NLP提出新技术后Code领域才有跟进的可能。个人感觉接下来在这个相对较小但很卷的领域的研究热点可能会慢慢向可解释性和模型分析（Analysis of Models &amp; Interpretability）方面转移，最近还研读了一些22年新出的Probing CodePTMs的文章，之后再补充。</p>
<p>希望本文对想熟悉代码表征预训练模型这块的朋友有所帮助～</p></div>

        <div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2022-07-09</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line print:!tw-hidden">
            <div class="post-info-md"></div>
            <div class="post-info-share"></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg>&nbsp;<a href="/blog-posts/tags/ptms/">PTMs</a>,&nbsp;<a href="/blog-posts/tags/codelms/">CodeLMs</a></section>
        <section class="print:!tw-hidden">
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/blog-posts/">Home</a></span>
        </section>
    </div>

    <div class="post-nav print:tw-hidden"><a href="/blog-posts/posts/2022-7-codeptms-review1/" class="prev" rel="prev" title="Code预训练语言模型学习指南（原理/分析/代码）Part1"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M31.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L127.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L201.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34z"/></svg>Code预训练语言模型学习指南（原理/分析/代码）Part1</a>
            <a href="/blog-posts/posts/first_post/" class="next" rel="next" title="OS Copilot">OS Copilot<svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a></div>
</div>
</article></div>
        </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreferrer" title="Hugo 0.124.1">Hugo</a>&nbsp;|&nbsp;Theme - <a href="https://github.com/HEIGE-PCloud/DoIt" target="_blank" rel="noopener noreferrer" title="DoIt 0.4.0"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1 0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7 0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174L402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7l-43.2-43.2c-4.1-4.1-10.8-4.1-14.8 0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg> DoIt</a>
                </div><div class="footer-line"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 448c-110.532 0-200-89.451-200-200 0-110.531 89.451-200 200-200 110.532 0 200 89.451 200 200 0 110.532-89.451 200-200 200zm107.351-101.064c-9.614 9.712-45.53 41.396-104.065 41.396-82.43 0-140.484-61.425-140.484-141.567 0-79.152 60.275-139.401 139.762-139.401 55.531 0 88.738 26.62 97.593 34.779a11.965 11.965 0 0 1 1.936 15.322l-18.155 28.113c-3.841 5.95-11.966 7.282-17.499 2.921-8.595-6.776-31.814-22.538-61.708-22.538-48.303 0-77.916 35.33-77.916 80.082 0 41.589 26.888 83.692 78.277 83.692 32.657 0 56.843-19.039 65.726-27.225 5.27-4.857 13.596-4.039 17.82 1.738l19.865 27.17a11.947 11.947 0 0 1-1.152 15.518z"/></svg>2024<span class="author">&nbsp;<a href="/blog-posts/" target="_blank" rel="noopener noreferrer"></a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer></div>

    <div id="fixed-buttons" class="print:!tw-hidden"><a href="#back-to-top" id="back-to-top-button" class="fixed-button" title="Back to Top">
            <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg>
        </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
            <svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 32C114.6 32 0 125.1 0 240c0 49.6 21.4 95 57 130.7C44.5 421.1 2.7 466 2.2 466.5c-2.2 2.3-2.8 5.7-1.5 8.7S4.8 480 8 480c66.3 0 116-31.8 140.6-51.4 32.7 12.3 69 19.4 107.4 19.4 141.4 0 256-93.1 256-208S397.4 32 256 32z"/></svg>
        </a>
    </div><div class="assets"><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{}};</script><script
    src="/blog-posts/lib/clipboard/clipboard.min.js"
    
  ></script><script
    src="/blog-posts/js/theme.min.js"
    
      defer
    
  ></script></div>
</body>

</html>
